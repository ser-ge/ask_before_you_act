{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "\n",
    "from oracle.oracle import Oracle, OracleWrapper\n",
    "from oracle.generator import gen_phrases\n",
    "from oracle.lang import parser, TreeToGrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = gen_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\n",
    "#     \"unseen \", \"empty \", \n",
    "    \"wall \", \"floor \", \"door \",\n",
    "         \"key \", \"box \", \"goal \", \"lava \", \"agent \",\n",
    "         \"red \", \"green \", \"blue \", \"purple \", \"yellow \", \n",
    "         \"grey \", \"north \", \"south \", \"west \", \"east \", \n",
    "         \"is \", \"is \", \"is \", \"is \", \"is \", \"is \", \"is \",\n",
    "         \"open \", \"closed \", \"locked \"]\n",
    "\n",
    "noun = [\n",
    "#     \"unseen \", \"empty \",\n",
    "        \"wall \", \"floor \", \"door \", \n",
    "        \"key \", \"ball \", \"box \", \"goal \", \"lava \", \"agent \"]\n",
    "adjective = [\"red \", \"green \", \"blue \", \"purple \", \"yellow \", \"grey \"]\n",
    "verb = [\"is \"]\n",
    "state = [\"open \", \"closed \", \"locked \"]\n",
    "direction = [\"north \", \"south \", \"west \", \"east \"]\n",
    "\n",
    "\n",
    "def reward_modifier(question):\n",
    "    words = question.split()\n",
    "    space_words = []\n",
    "    for item in words:\n",
    "        space_words.append(item + str(\" \"))\n",
    "    \n",
    "    reward = -5\n",
    "    if space_words[0] in adjective:\n",
    "        reward += 1\n",
    "    elif space_words[1] in noun:\n",
    "        reward += 1\n",
    "    elif space_words[2] in verb:\n",
    "        reward += 2\n",
    "    elif space_words[3] in state or space_words[3] in direction:\n",
    "        reward += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedCNN(nn.Module):\n",
    "    def __init__(self, action_dim=7, vocab_size=10):\n",
    "        super(SharedCNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size= vocab_size\n",
    "\n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy_head = nn.Linear(194, action_dim)  # 194 is 128 of hx + 2 of answer + 64 obs CNN\n",
    "        self.value_head = nn.Linear(194, 1)  # 194 is 128 of hx + 2 of answer + 64 obs CNN\n",
    "        \n",
    "        # CNN output is 64 dims\n",
    "        # Assuming qa_history is also 64 dimensional\n",
    "        self.question_rnn = nn.LSTMCell(self.vocab_size, 128) # (input_size, hidden_size)\n",
    "        self.question_head = nn.Linear(128, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, answer, hx, qa_history, flag=\"policy\"):\n",
    "        # Shared Body\n",
    "        x = x.view(-1, 3, 7, 7)  # x: (batch, C_in, H_in, W_in)\n",
    "        batch = x.shape[0]\n",
    "        x = self.image_conv(x).view(-1, 64)  # x: (batch, hidden)\n",
    "\n",
    "        # Split heads\n",
    "        if flag == \"policy\":\n",
    "            x = torch.cat((x, answer, hx), 1)\n",
    "            x_pol = self.policy_head(x)\n",
    "            return x_pol\n",
    "        elif flag == \"value\":\n",
    "            x = torch.cat((x, answer, hx), 1)\n",
    "            x_val = self.value_head(x)\n",
    "            return x_val\n",
    "        elif flag == \"question\":\n",
    "            hx = torch.cat((x,qa_history.view(-1,64)),1)\n",
    "            cx = torch.randn(hx.shape) # (batch, hidden_size)\n",
    "            x = torch.rand((1, self.vocab_size))  # First input is noise\n",
    "            output = []\n",
    "            entropy_qa = 0\n",
    "            log_probs_qa = []\n",
    "            for count in range(5):                \n",
    "                hx_tmp, cx_tmp = self.question_rnn(x, (hx, cx))\n",
    "                dist = self.softmax(self.question_head(hx_tmp))\n",
    "                m = distributions.Categorical(dist)\n",
    "                tkn_idx = m.sample()\n",
    "                log_probs_qa.append(m.log_prob(tkn_idx))\n",
    "                entropy_qa += m.entropy().item()\n",
    "                if count == 4: # We want the last hidden state to include the 4th word\n",
    "                    break\n",
    "                output.append(tkn_idx)\n",
    "                x = torch.zeros((batch, self.vocab_size))\n",
    "                x[:, tkn_idx] = 1\n",
    "\n",
    "                hx = hx_tmp.data\n",
    "                cx = cx_tmp.data\n",
    "\n",
    "            output = torch.stack(output, dim=0)\n",
    "            return output, hx_tmp, log_probs_qa, entropy_qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    def __init__(self, state_dim, action_dim, vocab, hidden_dim=64, learning_rate=0.001,\n",
    "                 gamma=0.99, clip_param=0.2, value_param=1, entropy_param=0.01,\n",
    "                 lmbda=0.95, backward_epochs=3):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "        self.model = SharedCNN(vocab_size=self.vocab_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.optimizer_qa = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.clip_param = clip_param\n",
    "        self.entropy_param = entropy_param\n",
    "        self.value_param = value_param\n",
    "        self.T = 1\n",
    "\n",
    "        self.done = True\n",
    "        self.data = []\n",
    "        self.backward_epochs = backward_epochs\n",
    "        \n",
    "    def ask(self, observation):\n",
    "        # Will this be taking the history of questions?\n",
    "        # And history of states?\n",
    "        qa_history = torch.rand((1,64))\n",
    "        observation = torch.FloatTensor(observation).to(device)\n",
    "        tkn_idxs, hx, log_probs_qa, entropy_qa = self.model(observation, _, _, qa_history, flag=\"question\")\n",
    "        output = str()\n",
    "        for word in tkn_idxs:\n",
    "            output += str(self.vocab[word])\n",
    "        return output, hx, log_probs_qa, entropy_qa\n",
    "\n",
    "    def act(self, observation, ans, hx):\n",
    "        # Calculate policy\n",
    "        observation = torch.FloatTensor(observation).to(device)\n",
    "        ans = torch.FloatTensor(ans).view((-1,2)).to(device)\n",
    "        logits = self.model(observation, ans, hx, _, flag=\"policy\")\n",
    "        action_prob = F.softmax(logits.squeeze()/self.T, dim=-1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        probs = action_prob[action]  # Policy log prob \n",
    "        entropy = dist.entropy()  # Entropy regularizer\n",
    "        return action.detach().item(), probs, entropy\n",
    "\n",
    "    def update(self):\n",
    "        obs, ans, hx, a, reward, next_obs, done, log_prob, entropy = self.get_batch()\n",
    "\n",
    "        for i in range(self.backward_epochs):\n",
    "            # Get current V\n",
    "            V_pred = self.model(obs, ans, hx, _, flag=\"value\").squeeze()\n",
    "            # Get next V\n",
    "            next_V_pred = self.model(next_obs, ans, hx, _, flag=\"value\").squeeze()\n",
    "\n",
    "            # Compute TD error\n",
    "            target = reward.squeeze().to(device) + self.gamma * next_V_pred * done.squeeze().to(device)\n",
    "            td_error = (target - V_pred).detach()\n",
    "\n",
    "            # Generalised Advantage Estimation\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta in reversed(td_error):\n",
    "                advantage = self.gamma * self.lmbda * advantage + delta\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            advantage = torch.FloatTensor(advantage_list).to(device)\n",
    "\n",
    "            # Clipped PPO Policy Loss\n",
    "            logits = self.model(obs, ans, hx, _, flag=\"policy\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            pi_a = probs.squeeze(1).gather(1, torch.LongTensor(a.long()).to(device))\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(log_prob))\n",
    "            surrogate1 = ratio * advantage\n",
    "            surrogate2 = advantage * torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param)\n",
    "            L_clip = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            # Entropy regularizer\n",
    "            L_entropy = self.entropy_param * entropy.detach().mean()\n",
    "\n",
    "            # Value function loss\n",
    "            L_value = self.value_param * F.smooth_l1_loss(V_pred, target.detach())\n",
    "\n",
    "            total_loss = -(L_clip - L_value + L_entropy).to(device)\n",
    "\n",
    "            # Update params\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            self.optimizer.step()\n",
    "        return total_loss.item()\n",
    "    \n",
    "    def update_QA(self, reward, log_prob, entropy):\n",
    "        for i in range(1):\n",
    "            # Reinforce Loss - TODO: check entropy parametr to avoid deterministic collapse\n",
    "            total_loss = -(reward * torch.cat(log_prob).mean() \n",
    "                            + 0.05*entropy).to(device)\n",
    "            # Update params\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            self.optimizer_qa.step()\n",
    "        return total_loss.item()\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def get_batch(self):\n",
    "        obs_batch = []\n",
    "        ans_batch = []\n",
    "        hx_batch = []\n",
    "        a_batch = []\n",
    "        r_batch = []\n",
    "        next_obs_batch = []\n",
    "        prob_batch = []\n",
    "        entropy_batch = []\n",
    "        done_batch = []\n",
    "        for transition in self.data:\n",
    "            obs, ans, hx, action, reward, next_obs, prob, entropy, done = transition\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            ans_batch.append(ans)\n",
    "            hx_batch.append(hx)\n",
    "            a_batch.append([action])\n",
    "            r_batch.append([reward])\n",
    "            next_obs_batch.append(next_obs)\n",
    "            prob_batch.append([prob])\n",
    "            entropy_batch.append([entropy])\n",
    "            done_bool = 0 if done else 1\n",
    "            done_batch.append([done_bool])\n",
    "        \n",
    "        obs = torch.FloatTensor(obs_batch).to(device)\n",
    "        ans = torch.FloatTensor(ans_batch).to(device)\n",
    "        hx = torch.cat(hx_batch)\n",
    "        a = torch.FloatTensor(a_batch).to(device)\n",
    "        r = torch.FloatTensor(r_batch).to(device)\n",
    "        next_obs = torch.FloatTensor(next_obs_batch).to(device)\n",
    "        prob = torch.FloatTensor(prob_batch).to(device)\n",
    "        entropy = torch.FloatTensor(entropy_batch).to(device)\n",
    "        done = torch.FloatTensor(done_batch).to(device)\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        return obs, ans, hx, a, r, next_obs, done, prob, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAEtrain(env, agent, exploration=True, n_episodes=1000,\n",
    "             log_interval=50, verbose=False, ID=False):\n",
    "    episode = 0\n",
    "    episode_loss = 0\n",
    "\n",
    "    episode_reward = []\n",
    "    loss_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    state = env.reset()['image']  # Discard other info\n",
    "    step = 0\n",
    "    \n",
    "    avg_syntax_r = 0\n",
    "\n",
    "    while episode < n_episodes:\n",
    "        # Ask - TODO pass qa_history\n",
    "        question, hx, log_probs_qa, entropy_qa = agent.ask(state)\n",
    "        \n",
    "        # Answer \n",
    "        ans, q_reward = env.answer(question)\n",
    "        q_reward += reward_modifier(question)/100\n",
    "        avg_syntax_r += 1/log_interval * (q_reward - avg_syntax_r)\n",
    "        \n",
    "        if reward_modifier(question) == 0:\n",
    "            print(question, ans)\n",
    "        \n",
    "        # Update Q&A - inner loop REINFORCE\n",
    "        # TODO: check entropy parameter to avoid deterministic collapse\n",
    "        agent.update_QA(q_reward, log_probs_qa, entropy_qa)\n",
    "\n",
    "        # Act\n",
    "        a, log_prob, entropy = agent.act(state, ans, hx)\n",
    "\n",
    "        # Step\n",
    "        next_state, r, done, _ = env.step(a)\n",
    "        if r > 0:\n",
    "            print(\"Goal reached\")\n",
    "            \n",
    "        r += q_reward\n",
    "        next_state = next_state['image']  # Discard other info\n",
    "        # Store\n",
    "        if exploration:\n",
    "            agent.store((state, ans, hx, a, r, next_state,\n",
    "                         log_prob.item(), entropy.item(), done))\n",
    "\n",
    "        # Advance\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "        # Logging\n",
    "        episode_reward.append(r)\n",
    "\n",
    "        if done:\n",
    "            # Update\n",
    "            if exploration:\n",
    "                episode_loss = agent.update()\n",
    "\n",
    "            state = env.reset()['image']  # Discard other info\n",
    "            step = 0\n",
    "\n",
    "            loss_history.append(episode_loss)\n",
    "            reward_history.append(sum(episode_reward))\n",
    "            episode_reward = []\n",
    "\n",
    "            episode += 1\n",
    "\n",
    "            if (episode) % log_interval == 0:\n",
    "                if verbose:\n",
    "                    avg_R = np.sum(reward_history[-log_interval:])/log_interval\n",
    "                    print(f\"Episode: {episode}, Reward: {avg_R:.2f}, Avg. syntax {avg_syntax_r:.3f}\")\n",
    "\n",
    "    return loss_history, reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== TRAINING - RUN 1/5 ==========================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f47c0c35c976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         lr, gamma, clip, value_param, entropy_param)\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     _, train_reward = GAEtrain(env, agent, exploration = True,\n\u001b[0m\u001b[1;32m     32\u001b[0m                                \u001b[0mn_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_eps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_log_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                 verbose = True)\n",
      "\u001b[0;32m<ipython-input-6-90e3deb25901>\u001b[0m in \u001b[0;36mGAEtrain\u001b[0;34m(env, agent, exploration, n_episodes, log_interval, verbose, ID)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7fa80af9a466>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, observation, ans, hx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Calculate policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"policy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "env = gym.make(\"MiniGrid-MultiRoom-N6-v0\")\n",
    "env = OracleWrapper(env, syntax_error_reward=0)  # MiniGrid-MultiRoom-N2-S4-v0, MiniGrid-Empty-5x5-v0\n",
    "env.seed(0)\n",
    "state_dim = env.observation_space['image'].shape\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Agent Params\n",
    "hidden_dim = 32\n",
    "lr = 0.0001\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "clip = 0.2\n",
    "entropy_param = 0.01\n",
    "value_param = 1\n",
    "\n",
    "N_eps = 1000\n",
    "train_log_interval = 10\n",
    "\n",
    "runs = 5\n",
    "\n",
    "# Store data for each run\n",
    "runs_reward = []\n",
    "\n",
    "for i in range(runs):\n",
    "    print(f\"========================== TRAINING - RUN {i+1:.0f}/{runs:.0f} ==========================\")\n",
    "    # Agent\n",
    "    agent = PPOAgent(state_dim, action_dim, vocab, hidden_dim, \n",
    "                        lr, gamma, clip, value_param, entropy_param)\n",
    "        \n",
    "    _, train_reward = GAEtrain(env, agent, exploration = True,\n",
    "                               n_episodes = N_eps, log_interval = train_log_interval,\n",
    "                                verbose = True)\n",
    "\n",
    "    # store result for every run\n",
    "    runs_reward.append(train_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = torch.FloatTensor(state['image'])\n",
    "agent.ask(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "# Helper\n",
    "def show_state(env, step=0):\n",
    "    plt.figure(3, figsize=(10, 10))\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    # pause for plots to update\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    plt.pause(0.0001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = OracleWrapper(gym.make(\"MiniGrid-Empty-8x8-v0\"))  \n",
    "env.seed(0)\n",
    "state_dim = env.observation_space['image'].shape\n",
    "action_dim = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obvs = env.reset()\n",
    "show_state(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward = env._answer(\"grey wall is north\")\n",
    "state\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have:\n",
    "env._answer(\"green door is closed\")\n",
    "\n",
    "# We want:\n",
    "env.step_oracle(\"green door is closed\")\n",
    "\n",
    "# answer - (0,0) (0,1) (1, 1)\n",
    "# reward -  -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "window = 10\n",
    "avg = pd.DataFrame(np.array(runs_reward)).T.rolling(window).mean().T\n",
    "\n",
    "df_reward_v2 = pd.DataFrame(avg).melt()\n",
    "sns.lineplot(ax=ax, x='variable', y='value', data=df_reward_v2)\n",
    "ax.set_title(f\"Reward training curve over {runs} runs\")\n",
    "ax.set_ylabel(f\"{window} episode moving average of mean agent\\'s reward\")\n",
    "ax.set_xlabel(\"Episodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
